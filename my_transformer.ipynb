{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (Forbidden).)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from easy_transformer import EasyTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, vocab_size=50257, init_range=0.02, n_ctx=1024, ln_eps=1e-05, n_heads=12, d_head=64, d_mlp=3072, n_layers=12, debug=True)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    vocab_size: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    ln_eps: float = 1e-5\n",
    "    n_heads: int = 12\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_layers: int = 12\n",
    "    debug: bool = True\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35])\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits_reference = reference_gpt2(tokens)\n",
    "print(logits_reference.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg)\n",
    "    random_input = torch.randn(shape)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg)\n",
    "    random_input = torch.randint(100, 1000, shape)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input_name, cache_dict=cache.cache_dict):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    # Allow inputs of strings or tensors\n",
    "    if isinstance(input_name, str):\n",
    "        reference_input = cache_dict[input_name]\n",
    "    else:\n",
    "        reference_input = input_name\n",
    "    print(\"Input shape:\", reference_input.shape)\n",
    "    output = layer(reference_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    reference_output = gpt2_layer(reference_input)\n",
    "    print(\"Reference output shape:\", reference_output.shape)\n",
    "\n",
    "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformes building blocks are:\n",
    "\n",
    "* Embeddings\n",
    "* Positional embeddings\n",
    "* LayerNorm\n",
    "* Attention\n",
    "* MLP\n",
    "* Unembed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens and Embeddings\n",
    "\n",
    "### Tokens\n",
    "* Tokens are subunits of text that are used to build the vocabulary of the model. Tokens are built with tokenizers, which are usually a BPE tokenizer.\n",
    "* BPE or byte pair encoding is an algorithm that builds a vocab based on the most frequent pairs of tokens in the text. It starts with a vocab of single characters and then merges the most frequent pairs of tokens until the desired vocab size is reached.\n",
    "* After the vocab is built, you'll have a mapping from tokens to integers.\n",
    "\n",
    "So given a text: \"Let's play this new game\"\n",
    "\n",
    "The tokenizer will output a list of integers: [1, 4, 39, 2, 45]\n",
    "\n",
    "Where 1 is the token for \"Let's\", 4 is the token for \"play\", 39 is the token for \"this\", 2 is the token for \"new\" and 45 is the token for \"game\". We're using a simple tokenizer ignoring punctuation, subwords and spaces. In a more realistic example, the word \"Let's\" would be tokenized as \"Let\" and \"'s\".\n",
    "\n",
    "### Embeddings\n",
    "* Embeddings are a lookup table that maps tokens to vectors in the embedding space\n",
    "* We have a vector in the embedding space for each token. So the embedding lookup table is a matrix $E \\in \\mathbb{R}^{V \\times d_{model}}$ where $V$ is the vocabulary size and $d_{model}$ is the embedding dimension.\n",
    "\n",
    "So in the example above we have a list of integers which we want to map to a list of vectors in the embedding space. Therefore tokens = [1, 4, 39, 2, 45] becomes embeddings = [e1, e4, e39, e2, e45] where $e_i \\in \\mathbb{R}^{d_{model}}$, where embeddings should have a shape of (sequence_length, d_model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: torch.Size([1, 5, 768])\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.W_E = nn.Parameter(torch.empty((config.vocab_size, config.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=config.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.W_E[tokens]\n",
    "        if self.config.debug: print(\"Embeddings:\", embeddings.shape)\n",
    "        return embeddings\n",
    "\n",
    "embeddings_layer = Embeddings(Config())\n",
    "tokens = torch.tensor([[1, 4, 39, 2, 45]])\n",
    "embeddings = embeddings_layer(tokens)\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Embeddings: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 5])\n",
      "Embeddings: torch.Size([1, 5, 768])\n",
      "Output shape: torch.Size([1, 5, 768])\n",
      "Reference output shape: torch.Size([1, 5, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "         [-0.0506, -0.1111,  0.1058,  ..., -0.1149,  0.0664,  0.0574],\n",
       "         [ 0.0746,  0.0573,  0.2295,  ..., -0.1254, -0.0506, -0.1640],\n",
       "         [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n",
       "         [-0.0240, -0.0773,  0.1979,  ..., -0.0814, -0.0701,  0.0289]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int_test(Embeddings, [2, 4])\n",
    "load_gpt2_test(Embeddings, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "* Positional embeddings are meant to give the model information about the order of the tokens in the sequence. \n",
    "* Intuitively, nearby tokens are more likely to be related to each other than distant tokens.\n",
    "* Positional embeddings are a lookup table that maps positions to vectors in the embedding space\n",
    "* This lookup table is a matrix $P \\in \\mathbb{R}^{n_{ctx} \\times d_{model}}$ where $n_{ctx}$ is the maximum sequence length and $d_{model}$ is the embedding dimension.\n",
    "* There are many ways to define positional embeddings:\n",
    "    * Learned embeddings: the positional embeddings are learned during training.\n",
    "    * Sine and cosine embeddings: the positional embeddings are defined as a function of the position index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 5])\n",
      "Output shape: torch.Size([1, 5, 768])\n",
      "Reference output shape: torch.Size([1, 5, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
       "           2.8267e-02,  5.4490e-02],\n",
       "         [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
       "           1.0172e-02, -1.5573e-04],\n",
       "         [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
       "           1.9325e-02, -2.1424e-02],\n",
       "         [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
       "           1.7659e-02, -7.0854e-03],\n",
       "         [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
       "           9.8542e-03, -7.0117e-03]]], grad_fn=<ExpandBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_pos = nn.Parameter(torch.empty(config.n_ctx, config.d_model))\n",
    "        nn.init.normal_(self.W_pos, std=config.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        pos_embeddings = self.W_pos[:tokens.size(1)]\n",
    "        pos_embeddings = einops.repeat(\n",
    "            pos_embeddings,\n",
    "            'position d_model -> batch position d_model',\n",
    "            batch=tokens.size(0)\n",
    "        )\n",
    "        return pos_embeddings\n",
    "\n",
    "rand_int_test(PositionalEmbeddings, [2, 4])\n",
    "load_gpt2_test(PositionalEmbeddings, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm\n",
    "* LayerNorm is a normalization layer that normalizes each vector in the input on the embedding dimension to have zero mean and unit variance\n",
    "* LayerNorm also applies a learnable scaling factor and bias to the normalized vector, enabling the model to learn scaling and shifting best suited for the data it's processing\n",
    "* LayerNorm helps with the stability of the training process and with the convergence of the model.\n",
    "    * To-do: Explore LayerNorm in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.w = nn.Parameter(torch.ones(config.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(config.d_model))\n",
    "    \n",
    "    def forward(self, residual):\n",
    "        residual -= einops.reduce(\n",
    "            residual, \n",
    "            'batch position d_model -> batch position 1',\n",
    "            'mean'\n",
    "        )\n",
    "        std = (\n",
    "            einops.reduce(\n",
    "                residual.pow(2),\n",
    "                \"batch position d_model -> batch position 1\", \"mean\"\n",
    "            ) + self.config.ln_eps\n",
    "        ).sqrt()\n",
    "        residual = residual / std\n",
    "        residual = residual * self.w + self.b\n",
    "        if self.config.debug: print(\"Normalized:\", residual.shape)\n",
    "        return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: torch.Size([1, 1024, 768])\n",
      "torch.Size([1, 1024, 768])\n",
      "tensor([[-4.9671e-09, -2.3594e-08, -8.6923e-09,  ...,  1.1176e-08,\n",
      "          9.9341e-09,  1.3659e-08]], grad_fn=<MeanBackward1>) tensor([[1.0006, 1.0006, 1.0006,  ..., 1.0006, 1.0006, 1.0006]],\n",
      "       grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_norm_layer = LayerNorm(Config())\n",
    "residual = torch.randn(1, 1024, 768)\n",
    "normalized_residual = layer_norm_layer(residual)\n",
    "print(normalized_residual.shape)\n",
    "print(normalized_residual.mean(dim=-1), normalized_residual.std(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(LayerNorm, [2, 4, 768])\n",
    "_ = load_gpt2_test(LayerNorm, reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "* The attention is a mechanism that allows each position in the sequence to consider the relevance of every other position in the sequence when computing a representation of that sequence. In other words, it allows the model to focus on the most relevant parts of the input sequence to predict the next token.\n",
    "* It does this by:\n",
    "    * KQ - Here we're generating the attention scores, which encode what source tokens are relevant to the destination tokens.\n",
    "    * We do this by applying a linear map from the input (embedding space) to the key space (K) and a linear map from the input (embedding space) to the query space (Q). Where the key space encodes the information we have and the query space encodes the information we're looking for.\n",
    "    * After generating K and Q, we compute the attention scores by taking the dot product of K and Q. Which measures how much the source tokens are relevant to the destination tokens.\n",
    "    * The next step is to apply a softmax function to the attention scores to get the attention weights. However, before applying softmax, since we're working with sequences, we need to mask the attention scores for the \"future\" tokens. Making sure that the model is causal.\n",
    "    * attn_scores @ V - Finally, we apply a linear map from the attention scores to the value space. Where the value space encodes the information we want to communicate to each destination token.\n",
    "        * K = Key -> Key is what do we contain\n",
    "            * Key is a linear map from the input space to the key space\n",
    "        * Q = Query -> Query is what we're looking for\n",
    "            * Query is a linear map from the input space to the query space\n",
    "        * V = Value -> Value is what we're communicating to each destination token\n",
    "            * Value is a linear map from the input space to the value space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.W_K = nn.Parameter(torch.empty(config.n_heads, config.d_model, config.d_head))\n",
    "        nn.init.normal_(self.W_K, std=config.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros(config.n_heads, config.d_head))\n",
    "\n",
    "        self.W_Q = nn.Parameter(torch.empty(config.n_heads, config.d_model, config.d_head))\n",
    "        nn.init.normal_(self.W_Q, std=config.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros(config.n_heads, config.d_head))\n",
    "\n",
    "        self.W_V = nn.Parameter(torch.empty(config.n_heads, config.d_model, config.d_head))\n",
    "        nn.init.normal_(self.W_V, std=config.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros(config.n_heads, config.d_head))\n",
    "\n",
    "        self.W_O = nn.Parameter(torch.empty(config.n_heads, config.d_head, config.d_model))\n",
    "        nn.init.normal_(self.W_O, std=config.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros(config.d_model))\n",
    "\n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        # linear map from batch, position, d_model -> batch, position, n_heads, d_head\n",
    "        q = einops.einsum(\n",
    "            normalized_resid_pre, self.W_Q,\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head'\n",
    "        ) + self.b_Q\n",
    "        k = einops.einsum(\n",
    "            normalized_resid_pre, self.W_K,\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head'\n",
    "        ) + self.b_K\n",
    "        # compute the attention scores\n",
    "        # batch, query_pos, n_heads, d_head, batch, key_pos, n_heads, d_head -> batch, query_pos, n_heads, key_pos\n",
    "        attn_scores = einops.einsum(\n",
    "            q, k,\n",
    "            'batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos'\n",
    "        )\n",
    "        attn_scores = attn_scores / math.sqrt(self.config.d_head)\n",
    "        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1)), diagonal=1).bool()\n",
    "        attn_scores = attn_scores.masked_fill(mask, -float('inf'))\n",
    "        attn_weights = attn_scores.softmax(dim=-1)\n",
    "\n",
    "        v = einops.einsum(\n",
    "            normalized_resid_pre, self.W_V,\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head'\n",
    "        ) + self.b_V\n",
    "        z = einops.einsum(\n",
    "            attn_weights, v,\n",
    "            'batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head'\n",
    "        )\n",
    "        attn_out = einops.einsum(\n",
    "            z, self.W_O,\n",
    "            'batch key_pos n_heads d_head, n_heads d_head d_model -> batch key_pos d_model'\n",
    "        ) + self.b_O\n",
    "        return attn_out\n",
    "\n",
    "attention_layer = Attention(Config())\n",
    "normalized_resid_pre = torch.randn(1, 5, 768)\n",
    "attn_weights = attention_layer(normalized_resid_pre)\n",
    "print(attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.9663e-01,  1.6985e-02,  3.4781e-02,  ...,  3.3119e-02,\n",
       "          -2.3129e-02,  1.8103e-01],\n",
       "         [ 1.3179e-03,  1.5750e-01, -1.4059e-01,  ..., -8.1998e-03,\n",
       "           5.3074e-03,  1.3511e-01],\n",
       "         [ 8.9738e-02, -7.2411e-01, -6.9866e-01,  ...,  5.5321e-02,\n",
       "           2.7958e-03,  9.0785e-02],\n",
       "         ...,\n",
       "         [-3.0286e-01,  4.9638e-02, -6.0990e-01,  ..., -3.7084e-02,\n",
       "          -4.9527e-04, -8.6008e-03],\n",
       "         [-1.0844e+00, -6.1457e-02,  2.2966e-01,  ..., -2.6688e-02,\n",
       "          -1.4368e-02,  3.3245e-02],\n",
       "         [ 3.7947e-01, -4.9886e-01,  2.6434e-01,  ..., -2.7894e-02,\n",
       "          -8.9027e-03,  4.8796e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(Attention, [2, 4, 768])\n",
    "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"blocks.0.ln1.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP - Multi Layer Perceptron\n",
    "* The MLP is a standard feedforward neural network, that performs a linear map, followed by a non-linearity, followed by another linear map.\n",
    "* So it's a single hidden layer neural network with a GeLu activation function. \n",
    "* Mapping from embedding space -> hidden space -> embedding space with the non-linearity in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "from easy_transformer.utils import gelu_new\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.W_in = nn.Parameter(torch.empty((config.d_model, config.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=config.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((config.d_mlp)))\n",
    "\n",
    "        self.W_out = nn.Parameter(torch.empty((config.d_mlp, config.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=config.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((config.d_model)))\n",
    "    \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        mlp_in = einops.einsum(\n",
    "            normalized_resid_pre, self.W_in,\n",
    "            'batch position d_model, d_model d_mlp -> batch position d_mlp'\n",
    "        ) + self.b_in\n",
    "        mlp_in = gelu_new(mlp_in)\n",
    "        mlp_out = einops.einsum(\n",
    "            mlp_in, self.W_out, \n",
    "            'batch position d_mlp, d_mlp d_model -> batch position d_model'\n",
    "        ) + self.b_out\n",
    "        return mlp_out\n",
    "\n",
    "mlp_layer = MLP(Config())\n",
    "normalized_resid_pre = torch.randn(1, 5, 768)\n",
    "mlp_out = mlp_layer(normalized_resid_pre)\n",
    "print(mlp_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4380,  0.3624,  0.5117,  ...,  1.7227,  1.5761,  0.0368],\n",
       "         [-1.0766, -0.0438,  0.3276,  ..., -0.5437,  0.4033,  0.3717],\n",
       "         [-1.2182, -1.5481, -0.9702,  ...,  1.0737,  0.7199,  0.5080],\n",
       "         ...,\n",
       "         [-0.4004,  0.8475,  0.2047,  ...,  0.3789,  0.0455, -0.4744],\n",
       "         [-0.0862,  0.7839,  0.9046,  ..., -0.2174, -0.5953,  0.8555],\n",
       "         [ 0.8448, -0.3743,  1.0397,  ...,  0.0296,  0.3405,  0.3585]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"blocks.0.ln2.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unembed\n",
    "* The unembed layer is a linear map from the embedding space to the logits space.\n",
    "* The logits space is the space of the unnormalized probabilities of the next token.\n",
    "* So the unembed layer outputs the logits for each token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 50257])\n"
     ]
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.empty((config.d_model, config.vocab_size)))\n",
    "        nn.init.normal_(self.W_U, std=config.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((config.vocab_size), requires_grad=False))\n",
    "    \n",
    "    def forward(self, normalized_resid_post):\n",
    "        logits = einops.einsum(\n",
    "            normalized_resid_post, self.W_U,\n",
    "            'batch position d_model, d_model vocab_size -> batch position vocab_size'\n",
    "        ) + self.b_U\n",
    "        return logits\n",
    "\n",
    "unembed_layer = Unembed(Config())\n",
    "normalized_resid_post = torch.randn(1, 5, 768)\n",
    "logits = unembed_layer(normalized_resid_post)\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -43.4317,  -39.8364,  -43.0660,  ...,  -54.0877,  -54.3452,\n",
       "           -42.3644],\n",
       "         [-128.0392, -127.9936, -130.7011,  ..., -136.7121, -129.9261,\n",
       "          -129.3965],\n",
       "         [-119.8521, -121.0064, -123.8820,  ..., -128.5181, -126.6028,\n",
       "          -121.9061],\n",
       "         ...,\n",
       "         [-112.9815, -112.7750, -117.0633,  ..., -121.2914, -117.6574,\n",
       "          -114.5005],\n",
       "         [ -98.6725, -104.4889, -108.7361,  ..., -118.3552, -113.8766,\n",
       "          -106.3604],\n",
       "         [-126.8285, -128.9596, -128.3941,  ..., -140.1969, -138.5882,\n",
       "          -122.3697]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: torch.Size([1, 5, 768])\n",
      "Normalized: torch.Size([1, 5, 768])\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config)\n",
    "        self.attn = Attention(config)\n",
    "        self.ln_2 = LayerNorm(config)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, resid_pre):\n",
    "        normalized_resid_pre = self.ln_1(resid_pre)\n",
    "        attn_out = self.attn(normalized_resid_pre)\n",
    "        resid_mid = resid_pre + attn_out\n",
    "        normalized_resid_mid = self.ln_2(resid_mid)\n",
    "        mlp_out = self.mlp(normalized_resid_mid)\n",
    "        resid_post = resid_mid + mlp_out\n",
    "        return resid_post\n",
    "\n",
    "transformer_block = TransformerBlock(Config())\n",
    "resid_pre = torch.randn(1, 5, 768)\n",
    "resid_post = transformer_block(resid_pre)\n",
    "print(resid_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Embeddings: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "\n",
      "Input shape: torch.Size([1, 111])\n",
      "Embeddings: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Normalized: torch.Size([1, 111, 768])\n",
      "Output shape: torch.Size([1, 111, 50257])\n",
      "Reference output shape: torch.Size([1, 111, 50257])\n",
      "0.13% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-64.2467, -62.7545, -65.4784,  ..., -72.5686, -70.3090, -64.5301],\n",
       "         [-21.9314, -21.9484, -23.8217,  ..., -28.2983, -27.8309, -22.8705],\n",
       "         [-47.7269, -47.1128, -50.2303,  ..., -55.1695, -53.4897, -47.8364],\n",
       "         ...,\n",
       "         [-88.1346, -86.2741, -90.8221,  ..., -97.1845, -94.9073, -89.3229],\n",
       "         [-60.6255, -59.4294, -63.5714,  ..., -68.8870, -67.3016, -61.2635],\n",
       "         [-74.9402, -73.2983, -77.2700,  ..., -83.5924, -80.9157, -75.2907]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = Embeddings(config)\n",
    "        self.pos_embed = PositionalEmbeddings(config)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        self.ln_final = LayerNorm(config)\n",
    "        self.unembed = Unembed(config)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        embeds = self.embed(tokens)\n",
    "        pos_embeds = self.pos_embed(tokens)\n",
    "        resid_pre = embeds + pos_embeds\n",
    "        for block in self.blocks:\n",
    "            resid_pre = block(resid_pre)\n",
    "        normalized_resid_final = self.ln_final(resid_pre)\n",
    "        out = self.unembed(normalized_resid_final)\n",
    "        return out\n",
    "\n",
    "rand_int_test(Transformer, [2, 4])\n",
    "load_gpt2_test(Transformer, reference_gpt2, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['blocks.0.ln_1.w', 'blocks.0.ln_1.b', 'blocks.0.ln_2.w', 'blocks.0.ln_2.b', 'blocks.1.ln_1.w', 'blocks.1.ln_1.b', 'blocks.1.ln_2.w', 'blocks.1.ln_2.b', 'blocks.2.ln_1.w', 'blocks.2.ln_1.b', 'blocks.2.ln_2.w', 'blocks.2.ln_2.b', 'blocks.3.ln_1.w', 'blocks.3.ln_1.b', 'blocks.3.ln_2.w', 'blocks.3.ln_2.b', 'blocks.4.ln_1.w', 'blocks.4.ln_1.b', 'blocks.4.ln_2.w', 'blocks.4.ln_2.b', 'blocks.5.ln_1.w', 'blocks.5.ln_1.b', 'blocks.5.ln_2.w', 'blocks.5.ln_2.b', 'blocks.6.ln_1.w', 'blocks.6.ln_1.b', 'blocks.6.ln_2.w', 'blocks.6.ln_2.b', 'blocks.7.ln_1.w', 'blocks.7.ln_1.b', 'blocks.7.ln_2.w', 'blocks.7.ln_2.b', 'blocks.8.ln_1.w', 'blocks.8.ln_1.b', 'blocks.8.ln_2.w', 'blocks.8.ln_2.b', 'blocks.9.ln_1.w', 'blocks.9.ln_1.b', 'blocks.9.ln_2.w', 'blocks.9.ln_2.b', 'blocks.10.ln_1.w', 'blocks.10.ln_1.b', 'blocks.10.ln_2.w', 'blocks.10.ln_2.b', 'blocks.11.ln_1.w', 'blocks.11.ln_1.b', 'blocks.11.ln_2.w', 'blocks.11.ln_2.b'], unexpected_keys=['blocks.0.ln1.w', 'blocks.0.ln1.b', 'blocks.0.ln2.w', 'blocks.0.ln2.b', 'blocks.0.attn.mask', 'blocks.0.attn.IGNORE', 'blocks.1.ln1.w', 'blocks.1.ln1.b', 'blocks.1.ln2.w', 'blocks.1.ln2.b', 'blocks.1.attn.mask', 'blocks.1.attn.IGNORE', 'blocks.2.ln1.w', 'blocks.2.ln1.b', 'blocks.2.ln2.w', 'blocks.2.ln2.b', 'blocks.2.attn.mask', 'blocks.2.attn.IGNORE', 'blocks.3.ln1.w', 'blocks.3.ln1.b', 'blocks.3.ln2.w', 'blocks.3.ln2.b', 'blocks.3.attn.mask', 'blocks.3.attn.IGNORE', 'blocks.4.ln1.w', 'blocks.4.ln1.b', 'blocks.4.ln2.w', 'blocks.4.ln2.b', 'blocks.4.attn.mask', 'blocks.4.attn.IGNORE', 'blocks.5.ln1.w', 'blocks.5.ln1.b', 'blocks.5.ln2.w', 'blocks.5.ln2.b', 'blocks.5.attn.mask', 'blocks.5.attn.IGNORE', 'blocks.6.ln1.w', 'blocks.6.ln1.b', 'blocks.6.ln2.w', 'blocks.6.ln2.b', 'blocks.6.attn.mask', 'blocks.6.attn.IGNORE', 'blocks.7.ln1.w', 'blocks.7.ln1.b', 'blocks.7.ln2.w', 'blocks.7.ln2.b', 'blocks.7.attn.mask', 'blocks.7.attn.IGNORE', 'blocks.8.ln1.w', 'blocks.8.ln1.b', 'blocks.8.ln2.w', 'blocks.8.ln2.b', 'blocks.8.attn.mask', 'blocks.8.attn.IGNORE', 'blocks.9.ln1.w', 'blocks.9.ln1.b', 'blocks.9.ln2.w', 'blocks.9.ln2.b', 'blocks.9.attn.mask', 'blocks.9.attn.IGNORE', 'blocks.10.ln1.w', 'blocks.10.ln1.b', 'blocks.10.ln2.w', 'blocks.10.ln2.b', 'blocks.10.attn.mask', 'blocks.10.attn.IGNORE', 'blocks.11.ln1.w', 'blocks.11.ln1.b', 'blocks.11.ln2.w', 'blocks.11.ln2.b', 'blocks.11.attn.mask', 'blocks.11.attn.IGNORE'])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_transformer = Transformer(Config(debug=False))\n",
    "my_transformer.load_state_dict(reference_gpt2.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on the the the the the the the the the the the the the un, the un the un, the un, the un, the un, the un a the the un, the un the un the un the the the the the un the the the the the the a the the the the the the the the def a the un a the the the the the un the the the the the the un the the the the the the a the a a the a a a a a a a a\n"
     ]
    }
   ],
   "source": [
    "text = \"Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on\"\n",
    "for i in range(100):\n",
    "    tokens = reference_gpt2.to_tokens(text)\n",
    "    logits_my_transformer = my_transformer(tokens)\n",
    "    next_token = logits_my_transformer[-1, -1].argmax()\n",
    "    text += reference_gpt2.tokenizer.decode(next_token)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on the the, the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embeddings): Embeddings()\n",
      "  (pos_embeddings): PositionalEmbeddings()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln_1): LayerNorm()\n",
      "      (attn): Attention()\n",
      "      (ln_2): LayerNorm()\n",
      "      (mlp): MLP()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm()\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(my_transformer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
