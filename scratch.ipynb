{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "* Backpropagation\n",
    "* y = xW + b\n",
    "* l = y - target\n",
    "\n",
    "* So we want to update W and b in order to minimize the loss\n",
    "* To minimize a function we need to find the derivative of the function\n",
    "* For example: if y > target, we need to decrease W and b. However, by how much?\n",
    "  * This question is even harder if you consider that W and b are tensors with potentially many elements to update\n",
    "  * So we need to find a way to tweak those values in a way that the loss is minimized\n",
    "  * We defined l(y, target) = y - target. However, y is a function of W and b\n",
    "  * So l((W, b), target) = (Wx + b) - target\n",
    "  * This is clearly a DAG (Directed Acyclic Graph)!\n",
    "  * l value depends on y, so l -> y\n",
    "  * y dependends on W and b, so l -> y -> W and l -> y -> b\n",
    "  * So with this graph we can understand how changes in W and b affect l\n",
    "  * The change in l due to a change in y is dl/dy\n",
    "  * The change of l due to a change in W is:\n",
    "    * The change of l due to a change in y (let's call it dl/dy) AND\n",
    "    * The change of y due to a change in W (let's call it dy/dW)\n",
    "    * Therefore dl/dW = dl/dy * dy/dW\n",
    "  * The change of l due to a change in b is the same:\n",
    "    * dl/db = dl/dy * dy/db\n",
    "\n",
    "More generically, for every node in the backpropagation graph, if we have:\n",
    "* l -> y -> a -> b\n",
    "* ith node gradient is output_gradient * local_gradient\n",
    "* dl/da = dl/dy * dy/da\n",
    "* dl/db = dl/da * da/db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "l = (xW + b) - target\n",
    "dl/dy = 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
